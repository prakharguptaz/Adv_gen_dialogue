{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = None\n",
    "MASK_CLS = 'ilm.mask.hierarchical.MaskHierarchical'\n",
    "# MASK_CLS = 'ilm.mask.hierarchical_dailydialog.MaskHierarchical'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download pretrained model\n",
    "\n",
    "if MODEL_DIR is None:\n",
    "#     !python acl20_repro.py model sto ilm | bash\n",
    "#     MODEL_DIR = '/tmp/ilm/models/sto_ilm'\n",
    "    MODEL_DIR = 'train'\n",
    "#     MODEL_DIR = 'train-naive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'ilm.mask.hierarchical.MaskHierarchical'>\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import os\n",
    "import random\n",
    "import importlib\n",
    "import pickle\n",
    "import sys\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from ilm.datasets import Dataset, get_dataset\n",
    "import ilm.mask\n",
    "from ilm.mask.util import mask_cls_str_to_type\n",
    "from ilm.mask.util import apply_masked_spans\n",
    "from ilm.mask.util import masked_spans_bounds_valid, masked_spans_overlap\n",
    "from create_ilm_examples import randomly_mask_dataset, randomly_mask_document\n",
    "\n",
    "# import importlib\n",
    "# importlib.reload(create_ilm_examples)\n",
    "# Set seed\n",
    "seed = 0\n",
    "if seed is None:\n",
    "    seed = random.randint(0, 1e6)\n",
    "    print('Random seed {}'.format(seed))\n",
    "    random.seed(seed)\n",
    "\n",
    "mask_type = mask_cls_str_to_type(MASK_CLS)\n",
    "print(mask_type)\n",
    "masker = mask_type(0.25, mask_paragraph_p=0.0, mask_document_p=0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3919, 837, 1312, 17666, 1279, 91, 10745, 359, 62, 4775, 91, 6927, 91, 10745, 359, 62, 4775, 91, 29, 764, 27, 91, 10745, 359, 62, 34086, 594, 91, 29]\n",
      "[3919, 837, 1312, 17666, 1279, 91, 10745, 359, 62, 4775, 91, 6927, 91, 10745, 359, 62, 4775, 91, 29, 764, 27, 91, 10745, 359, 62, 34086, 594, 91, 29]\n",
      "[3919, 837, 1312, 17666, 1279, 91, 10745, 359, 62, 4775, 91, 6927, 91, 10745, 359, 62, 4775, 91, 29, 764, 27, 91, 10745, 359, 62, 34086, 594, 91, 29]\n",
      "{'<|startofinfill|>': 50257, '<|endofinfill|>': 50258, '<|speaker1|>': 50259, '<|speaker2|>': 50260, '<|context|>': 50261, '<|endofcontext|>': 50262, '<|response|>': 50263, '<|infill_document|>': 50264, '<|infill_paragraph|>': 50265, '<|infill_sentence|>': 50266, '<|infill_ngram|>': 50267, '<|infill_word|>': 50268} 50269\n"
     ]
    }
   ],
   "source": [
    "# Prepare tokenizer\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import ilm.tokenize_util\n",
    "\n",
    "tokenizer = ilm.tokenize_util.Tokenizer.GPT2\n",
    "with open(os.path.join(MODEL_DIR, 'additional_ids_to_tokens.pkl'), 'rb') as f:\n",
    "    additional_ids_to_tokens = pickle.load(f)\n",
    "additional_tokens_to_ids = {v:k for k, v in additional_ids_to_tokens.items()}\n",
    "print(ilm.tokenize_util.encode('no , i dont <|infill_word|><|infill_word|> .<|infill_sentence|>', tokenizer))\n",
    "try:\n",
    "    lenvocab = ilm.tokenize_util.update_tokenizer(additional_ids_to_tokens, tokenizer)\n",
    "except ValueError:\n",
    "    print('Already updated')\n",
    "print(ilm.tokenize_util.encode('no , i dont <|infill_word|><|infill_word|> .<|infill_sentence|>', tokenizer))\n",
    "doc_tokens = (ilm.tokenize_util.tokenize('no , i dont <|infill_word|><|infill_word|> .<|infill_sentence|>', tokenizer))\n",
    "doc_tokens_ids = ilm.tokenize_util.tokens_to_ids(doc_tokens, tokenizer=tokenizer)\n",
    "print(doc_tokens_ids)\n",
    "\n",
    "print(additional_tokens_to_ids, lenvocab)\n",
    "\n",
    "tokenizer_state = ilm.tokenize_util._get_tokenizer_state(tokenizer)\n",
    "# for k,v in additional_tokens_to_ids.items():\n",
    "#     tokenizer.add_special_tokens([k])\n",
    "# tokenizer_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GPT2LMHeadModel.from_pretrained(MODEL_DIR)\n",
    "model.eval()\n",
    "_ = model.to(device)\n",
    "\n",
    "from lm_scorer.models.auto import AutoLMScorer as LMScorer\n",
    "\n",
    "# # Available models\n",
    "# list(LMScorer.supported_model_names())\n",
    "# # => [\"gpt2\", \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\", distilgpt2\"]\n",
    "\n",
    "# # Load model to cpu or cuda\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "batch_size = 1\n",
    "scorer = LMScorer.from_pretrained(\"gpt2\", device=device, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|context|> <speaker1> while the curtains are being made , i can start having people look at the kitchen . i can't stand that old kitchen . i won'tbe able to cook there . i don't want to use that electric stove . <speaker2> we need to find an interior decorating company to redecorate the kitchen . i believe in portland there are shops that specialize in kitchen renovation . i will look in the yellow pages .i'd like a kitchen mostly in ivory and light green . <speaker1> i agree . the colors must be soft and pleasant . you should feel comfortable when you cook our dinners . <speaker2>me ? cook our dinners ? hah ! you will be cooking , dear . you will cook . <|end of context|> <|response|> <|speaker2|> no , i don't <|infill_word|> <|infill_word|> in my job .<|infill_sentence|>\n",
      "[3919, 837, 1312, 17666, 1279, 91, 10745, 359, 62, 4775, 91, 29, 1279, 91, 10745, 359, 62, 4775, 91, 29, 764, 1279, 91, 10745, 359, 62, 34086, 594, 91, 29]\n"
     ]
    }
   ],
   "source": [
    "# Create context\n",
    "\n",
    "context = \"\"\"\n",
    "Math Class\n",
    "Chris was bad at _. _ _ _ He ended up passing the test.\n",
    "\"\"\".strip()\n",
    "\n",
    "context = \"\"\"<|context|> <speaker1> while the curtains are being made , i can start having people look at the kitchen . i can't stand that old kitchen . i won't\\\n",
    "be able to cook there . i don't want to use that electric stove . <speaker2> we need to find an interior decorating company to redecorate the kitchen . i believe in portland there are shops that specialize in kitchen renovation . i will look in the yellow pages .\\\n",
    "i'd like a kitchen mostly in ivory and light green . <speaker1> i agree . the colors must be soft and pleasant . you should feel comfortable when you cook our dinners . <speaker2>\\\n",
    "me ? cook our dinners ? hah ! you will be cooking , dear . you will cook . <|end of context|> <|response|> <|speaker2|> no , i don't  _  _ in my job . _\"\"\".strip()\n",
    "\n",
    "\n",
    "\n",
    "context_ids = ilm.tokenize_util.encode(context, tokenizer)\n",
    "\n",
    "# Replace blanks with appropriate tokens from left to right\n",
    "_blank_id = ilm.tokenize_util.encode(' _', tokenizer)[0]\n",
    "# context_ids[context_ids.index(_blank_id)] = additional_tokens_to_ids['<|infill_word|>']\n",
    "# context_ids[context_ids.index(_blank_id)] = additional_tokens_to_ids['<|infill_sentence|>']\n",
    "# context_ids[context_ids.index(_blank_id)] = additional_tokens_to_ids['<|infill_sentence|>']\n",
    "# context_ids[context_ids.index(_blank_id)] = additional_tokens_to_ids['<|infill_sentence|>']\n",
    "\n",
    "context_ids[context_ids.index(_blank_id)] = additional_tokens_to_ids['<|infill_word|>']\n",
    "context_ids[context_ids.index(_blank_id)] = additional_tokens_to_ids['<|infill_word|>']\n",
    "context_ids[context_ids.index(_blank_id)] = additional_tokens_to_ids['<|infill_sentence|>']\n",
    "\n",
    "\n",
    "print(ilm.tokenize_util.decode(context_ids, tokenizer))\n",
    "# print(context_ids)\n",
    "\n",
    "print(ilm.tokenize_util.encode('no , i dont <|infill_word|> <|infill_word|> . <|infill_sentence|>', tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|context|> <speaker1> while the curtains are being made , i can start having people look at the kitchen . i can't stand that old kitchen . i won'tbe able to cook there . i don't want to use that electric stove . <speaker2> we need to find an interior decorating company to redecorate the kitchen . i believe in portland there are shops that specialize in kitchen renovation . i will look in the yellow pages .i'd like a kitchen mostly in ivory and light green . <speaker1> i agree . the colors must be soft and pleasant . you should feel comfortable when you cook our dinners . <speaker2>me ? cook our dinners ? hah ! you will be cooking , dear . you will cook . <|end of context|> <|response|> <|speaker2|> no , i don't <|infill_word|> <|infill_word|> in my job .<|infill_sentence|>\n",
      "begin\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "<|context|> <speaker1> while the curtains are being made , i can start having people look at the kitchen . i can't stand that old kitchen . i won'tbe able to cook there . i don't want to use that electric stove . <speaker2> we need to find an interior decorating company to redecorate the kitchen . i believe in portland there are shops that specialize in kitchen renovation . i will look in the yellow pages .i'd like a kitchen mostly in ivory and light green . <speaker1> i agree . the colors must be soft and pleasant . you should feel comfortable when you cook our dinners . <speaker2>me ? cook our dinners ? hah ! you will be cooking , dear . you will cook . <|end of context|> <|response|> <|speaker2|> no , i don't  .  in in my job . you have to be creative .\n",
      "--------------------------------------------------------------------------------\n",
      "<|context|> <speaker1> while the curtains are being made , i can start having people look at the kitchen . i can't stand that old kitchen . i won'tbe able to cook there . i don't want to use that electric stove . <speaker2> we need to find an interior decorating company to redecorate the kitchen . i believe in portland there are shops that specialize in kitchen renovation . i will look in the yellow pages .i'd like a kitchen mostly in ivory and light green . <speaker1> i agree . the colors must be soft and pleasant . you should feel comfortable when you cook our dinners . <speaker2>me ? cook our dinners ? hah ! you will be cooking , dear . you will cook . <|end of context|> <|response|> <|speaker2|> no , i don't  cook  occasionally in my job . i just finish my dinner automatically .\n",
      "--------------------------------------------------------------------------------\n",
      "<|context|> <speaker1> while the curtains are being made , i can start having people look at the kitchen . i can't stand that old kitchen . i won'tbe able to cook there . i don't want to use that electric stove . <speaker2> we need to find an interior decorating company to redecorate the kitchen . i believe in portland there are shops that specialize in kitchen renovation . i will look in the yellow pages .i'd like a kitchen mostly in ivory and light green . <speaker1> i agree . the colors must be soft and pleasant . you should feel comfortable when you cook our dinners . <speaker2>me ? cook our dinners ? hah ! you will be cooking , dear . you will cook . <|end of context|> <|response|> <|speaker2|> no , i don't  like  cooking in my job . i would rather cook myself to some excellent meal .\n",
      "--------------------------------------------------------------------------------\n",
      "<|context|> <speaker1> while the curtains are being made , i can start having people look at the kitchen . i can't stand that old kitchen . i won'tbe able to cook there . i don't want to use that electric stove . <speaker2> we need to find an interior decorating company to redecorate the kitchen . i believe in portland there are shops that specialize in kitchen renovation . i will look in the yellow pages .i'd like a kitchen mostly in ivory and light green . <speaker1> i agree . the colors must be soft and pleasant . you should feel comfortable when you cook our dinners . <speaker2>me ? cook our dinners ? hah ! you will be cooking , dear . you will cook . <|end of context|> <|response|> <|speaker2|> no , i don't  ever  cook in my job . i think it's too hard for us to make the most of the food we eat .\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(ilm.infer)\n",
    "\n",
    "from ilm.infer import infill_with_ilm, infill_naive_with_ilm\n",
    "\n",
    "infill_function = infill_with_ilm\n",
    "# infill_function = infill_naive_with_ilm\n",
    "\n",
    "\n",
    "test_s = (ilm.tokenize_util.decode(context_ids, tokenizer))\n",
    "print(test_s)\n",
    "etest_s =ilm.tokenize_util.encode(test_s, tokenizer)\n",
    "# print(etest_s)\n",
    "test_ss = (ilm.tokenize_util.decode(etest_s, tokenizer))\n",
    "# print(context_ids)\n",
    "# print(etest_s)\n",
    "# print(test_s)\n",
    "# print(test_ss)\n",
    "print('begin\\n')\n",
    "generated = infill_function(\n",
    "    model,\n",
    "    additional_tokens_to_ids,\n",
    "    context_ids,\n",
    "    num_infills=4)\n",
    "for g in generated:\n",
    "    print('-' * 80)\n",
    "    print(ilm.tokenize_util.decode(g, tokenizer))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Encourage your child in her interests and hobbies even if they are things that you know little about.', 'Encourage your child in her interests and <|MaskHierarchicalType.WORD|> even <|MaskHierarchicalType.WORD|> they are things <|MaskHierarchicalType.WORD|> you know little about.'], ['Encourage your child in her interests and hobbies even if they are things that you know little about.', 'Encourage your child in <|MaskHierarchicalType.NGRAM|> and hobbies even if they <|MaskHierarchicalType.NGRAM|> <|MaskHierarchicalType.NGRAM|>.'], ['Encourage your child in her interests and hobbies even if they are things that you know little about.', 'Encourage your child in her interests and hobbies even if they are things that you know little about. <|MaskHierarchicalType.SENTENCE|>']]\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import sys\n",
    "importlib.reload(sys.modules['create_ilm_examples'])\n",
    "from create_ilm_examples import randomly_mask_dataset, randomly_mask_document\n",
    "\n",
    "\n",
    "docs_masked = []\n",
    "\n",
    "error_to_count_total = Counter()\n",
    "num_examples_per_document=3\n",
    "max_num_retries=5\n",
    "\n",
    "# docs = [' we need to find an interior decorating company to redecorate the kitchen . i believe in portland there are shops that specialize in kitchen renovation .']\n",
    "docs_in = ['Encourage your child in her interests and hobbies even if they are things that you know little about.']\n",
    "# docs_in = ['Encourage know little about.']\n",
    "\n",
    "num_retries_total = 0\n",
    "\n",
    "def get_masked_spans(docs):\n",
    "    result = []\n",
    "    docs_masked=[]\n",
    "#     for doc in tqdm(docs):\n",
    "    for doc in (docs):\n",
    "        doc_masks, error_to_count = randomly_mask_document(\n",
    "            doc,\n",
    "            masker,\n",
    "            num_examples_per_document,\n",
    "            max_num_retries=max_num_retries,\n",
    "              min_masked_spans=None,\n",
    "              max_masked_spans=None,\n",
    "              random_sample_down_to_max=True,\n",
    "              ensure_valid_bounds_in_spans=True,\n",
    "              ensure_nonoverlapping_spans=True,\n",
    "            )\n",
    "#         print(doc_masks, error_to_count)\n",
    "        docs_masked.append((doc, doc_masks))\n",
    "        for k, v in error_to_count.items():\n",
    "            error_to_count_total[k] += v\n",
    "#     print(len(docs_masked[0]))\n",
    "    i = 0\n",
    "    for doc, examples in docs_masked:\n",
    "#         if len(examples) == 0:\n",
    "#             continue\n",
    "\n",
    "        for masked_spans in examples:\n",
    "#         masked_spans = random.choice(examples)\n",
    "            mask_span_type_to_str = {t:'<|{}|>'.format(str(t)) for t, _, _ in masked_spans}\n",
    "            context, answers = apply_masked_spans(\n",
    "                doc,\n",
    "                masked_spans,\n",
    "                mask_span_type_to_str)\n",
    "            context_copy = context\n",
    "            if context_copy in ['<|MaskHierarchicalType.DOCUMENT|>', '<|MaskHierarchicalType.PARAGRAPH|>', '<|MaskHierarchicalType.SENTENCE|>']:\n",
    "                if random.random() >0.5: context_copy = doc + ' ' + context\n",
    "                else: context_copy = context + ' ' + doc\n",
    "            result.append([doc,context_copy])\n",
    "#             for _ in range(4):\n",
    "#               print('-' * 80)\n",
    "#             print(' ' * 36 + '-' * 8)\n",
    "#             print(' ' * 36 + 'ORIGINAL')\n",
    "#             print(' ' * 36 + '-' * 8)\n",
    "#             print(doc)\n",
    "#             print(' ' * 36 + '-' * 7)\n",
    "#             print(' ' * 36 + 'CONTEXT')\n",
    "#             print(' ' * 36 + '-' * 7)\n",
    "#             print(context)\n",
    "#             print(' ' * 36 + '-' * 7)\n",
    "#             print(' ' * 36 + 'ANSWERS')\n",
    "#             print(' ' * 36 + '-' * 7)\n",
    "#             for i, (span_type, span) in enumerate(answers):\n",
    "#               print(mask_span_type_to_str[span_type])\n",
    "#               print(span)\n",
    "#               if i != len(answers) - 1:\n",
    "#                 print('-' * 20)\n",
    "\n",
    "        return result\n",
    "\n",
    "outs = get_masked_spans(docs_in)\n",
    "print(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " <|infill_ngram|> in her interests and hobbies even if they <|infill_ngram|> know little <|infill_ngram|> .\n",
      " ok , you sound in her interests and hobbies even if they  aren't your know little  about .\n",
      "\n",
      "Encourage your child in her interests <|infill_ngram|> <|infill_ngram|> little about.\n",
      "Encourage your child in her interests  .  so long as she is told to be good little about.\n",
      "\n",
      " <|infill_sentence|> Encourage your child in her interests and hobbies even if they are things that you know little about.\n",
      " when i cook , i will put my clothes on . Encourage your child in her interests and hobbies even if they are things that you know little about.\n",
      "\n",
      "Encourage <|infill_ngram|> <|infill_ngram|> interests and hobbies <|infill_ngram|> know little about.\n",
      "Encourage  everyone to be responsible for their own diet and house cleaning .  the kitchen promotes healthy interests and hobbies  are full of healthy recipes . know little about.\n",
      "\n",
      " <|infill_ngram|> in <|infill_ngram|> even if they <|infill_ngram|> things <|infill_ngram|> you know little about.\n",
      " my baby ! in  the kitchen . even if they  do use the same things  ? you know little about.\n",
      "\n",
      "5\n",
      "['<|MaskHierarchicalType.NGRAM|> in her interests and hobbies even if they <|MaskHierarchicalType.NGRAM|> know little <|MaskHierarchicalType.WORD|>.', 'Encourage your child in her interests <|MaskHierarchicalType.NGRAM|> <|MaskHierarchicalType.NGRAM|> little about.', '<|MaskHierarchicalType.SENTENCE|> Encourage your child in her interests and hobbies even if they are things that you know little about.', 'Encourage <|MaskHierarchicalType.WORD|> <|MaskHierarchicalType.NGRAM|> interests and hobbies <|MaskHierarchicalType.NGRAM|> know little about.', '<|MaskHierarchicalType.NGRAM|> in <|MaskHierarchicalType.NGRAM|> even if they <|MaskHierarchicalType.WORD|> things <|MaskHierarchicalType.WORD|> you know little about.']\n",
      "[\" ok , you sound in her interests and hobbies even if they  aren't your know little  about .\", 'Encourage your child in her interests  .  so long as she is told to be good little about.', ' when i cook , i will put my clothes on . Encourage your child in her interests and hobbies even if they are things that you know little about.', 'Encourage  everyone to be responsible for their own diet and house cleaning .  the kitchen promotes healthy interests and hobbies  are full of healthy recipes . know little about.', ' my baby ! in  the kitchen . even if they  do use the same things  ? you know little about.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "context = \"\"\"<|context|> <speaker1> while the curtains are there , i can start having people look at the kitchen . i can't stand that old kitchen . i won't\\\n",
    "be able to cook there . i don't want to use that electric stove . <speaker2> we need to find an interior decorating company to redecorate the kitchen . i believe in portland there are shops that specialize in kitchen renovation . i will look in the yellow pages .\\\n",
    "i'd like a kitchen mostly in ivory and light green . <speaker1> i agree . the colors must be soft and pleasant . you should feel comfortable when you cook our dinners . <speaker2>\\\n",
    " me ? cook our dinners ? hah ! you will be cooking , dear . you will cook . <|end of context|> <|response|> <|speaker2|> \"\"\".strip()\n",
    "\n",
    "stop_words = ['another', 'once', 'thereupon', 'whom', 'regarding', 'first', 'anyhow', 'whence', 'else', 'might', 'themselves', '’s', 'of', 'side', \"'m\", 'top', 'ours', 'will', 'whole', 'sixty', 'if', 'but', 'serious', 'cannot', 'became', 'about', 'do', 'take', 'an', 'being', 'throughout', 'after', 're', 'were', 'must', 'thence', 'whether', 'whereafter', 'hundred', 'again', 'still', 'further', 'above', 'third', 'them', 'any', 'why', \"'ll\", 'wherein', 'towards', 'every', 'five', 'most', 'into', 'meanwhile', 'may', 'onto', 'neither', 'namely', 'fifteen', 'i', 'below', 'they', 'without', 'him', 'never', 'give', 'forty', 'own', 'thus', 'whereby', 'yourself', 'itself', 'somewhere', 'via', 'full', 'next', 'been', 'always', 'put', 'whereupon', 'because', 'so', 'under', 'during', 'than', 'several', 'upon', 'very', '’d', 'something', \"n't\", 'ten', '‘m', 'though', 'anything', 'fifty', 'all', 'seemed', 'well', 'twenty', 'more', 'amongst', 'wherever', 'name', 'am', 'therein', 'much', 'among', 'less', 'when', 'except', 'hereafter', 'has', 'along', 'seems', 'now', 'up', 'sometimes', 'alone', 'ca', 'everything', 'enough', 'himself', 'everyone', '‘ve', 'quite', '‘re', 'elsewhere', 'whoever', 'it', 'back', 'me', 'otherwise', 'perhaps', 'latter', 'on', 'already', 'across', 'whither', 'what', 'within', '‘d', 'n’t', \"'ve\", 'that', 'nevertheless', 'someone', 'nowhere', 'empty', 'out', 'some', 'really', 'off', 'each', 'mostly', 'hence', 'yet', 'are', 'using', 'nothing', 'yourselves', 'no', \"'re\", 'besides', 'over', '‘ll', 'sometime', 'becomes', 'before', 'anywhere', 'by', 'seem', 'for', 'us', 'where', 'many', 'these', 'he', 'toward', 'her', 'should', 'doing', 'ever', 'nor', 'three', 'between', 'can', 'same', 'whereas', 'until', 'either', 'their', 'due', '’ll', 'beside', 'few', 'the', 'was', 'which', 'its', 'just', 'our', 'your', 'say', 'noone', 'front', 'against', 'down', 'such', 'anyway', 'also', 'everywhere', 'two', 'together', 'and', 'others', 'bottom', 'eight', 'we', 'my', \"'d\", 'whatever', 'six', 'indeed', 'did', 'other', 'becoming', 'afterwards', 'from', 'thereafter', 'too', 'you', 'behind', 'mine', 'a', 'thereby', 'not', 'to', 'nobody', 'be', 'done', 'then', 'at', 'even', '’re', 'here', 'various', 'make', 'twelve', 'how', 'as', 'since', 'there', 'call', 'somehow', 'she', 'in', 'anyone', 'almost', 'moreover', 'beyond', 'herself', 'yours', 'hereby', 'both', 'nine', 'latterly', 'herein', 'have', 'eleven', 'while', 'his', 'please', 'n‘t', 'move', 'those', 'get', 'could', 'beforehand', 'this', 'is', 'per', 'although', 'hers', 'made', '’m', 'often', 'ourselves', 'therefore', 'whose', 'keep', 'only', 'none', 'seeming', 'one', '’ve', 'hereupon', 'whenever', 'unless', '‘s', 'does', 'had', 'would', 'however', 'formerly', 'see', 'used', 'show', 'around', 'part', \"'s\", 'become', 'least', 'thru', 'last', 'who', 'rather', 'myself', 'through', 'former', 'four', 'or', 'with', 'go']\n",
    "\n",
    "def get_infill_substitutes(sentence):\n",
    "#     '<|infill_document|>': 50259, '<|infill_paragraph|>': 50260, '<|infill_sentence|>': 50261, '<|infill_ngram|>': 50262, '<|infill_word|>\n",
    "    sentence = sentence.replace('<|MaskHierarchicalType.PARAGRAPH|>', ' <|infill_sentence|> ')\n",
    "    sentence = sentence.replace('<|MaskHierarchicalType.DOCUMENT|>', ' <|infill_sentence|> ')\n",
    "    sentence = sentence.replace('<|MaskHierarchicalType.SENTENCE|>', ' <|infill_sentence|> ')\n",
    "    sentence = sentence.replace('<|MaskHierarchicalType.NGRAM|>', ' <|infill_ngram|> ')\n",
    "#     sentence = sentence.replace('<|MaskHierarchicalType.WORD|>', '<|infill_word|>')\n",
    "    #word replacement with ngram to make it longer\n",
    "    sentence = sentence.replace('<|MaskHierarchicalType.WORD|>', ' <|infill_ngram|> ')\n",
    "    sentence = sentence.replace('   ', ' ')\n",
    "    sentence = sentence.replace('  ', ' ')\n",
    "    return sentence\n",
    "\n",
    "_blank_id = ilm.tokenize_util.encode(' _', tokenizer)[0]\n",
    "\n",
    "def get_context_ids(data_input):\n",
    "    \n",
    "    types_of_blanks = ['<|infill_document|>', '<|infill_paragraph|>', '<|infill_sentence|>', '<|infill_ngram|>', '<|infill_word|>']\n",
    "    \n",
    "    list_of_blanks = []\n",
    "    s = data_input\n",
    "    while True:\n",
    "        m = re.search('\\<\\|infill_(.+?)\\|\\>',s)\n",
    "        if m:\n",
    "            found = m.group(1)\n",
    "            list_of_blanks.append('<|infill_'+found+'|>')\n",
    "            s = s.replace('<|infill_'+found+'|>', ' _',1)\n",
    "        else:\n",
    "            break\n",
    "    data_input = s\n",
    "    \n",
    "    context_ids = ilm.tokenize_util.encode(data_input, tokenizer)\n",
    "    for type_blank in list_of_blanks:\n",
    "        context_ids[context_ids.index(_blank_id)] = additional_tokens_to_ids[type_blank]\n",
    "\n",
    "    return context_ids\n",
    "\n",
    "def test_pure_infill(infill_sent):\n",
    "    infills = re.findall('\\<\\|infill_(.+?)\\|\\>',infill_sent)\n",
    "    words = infill_sent.split()\n",
    "#     print(len(infills), len(words))\n",
    "    if len(words)<=len(infills):\n",
    "        return True\n",
    "    \n",
    "    if len(infills)<=1:\n",
    "        if len(infills)==1 and infills[0] in ['<|infill_ngram|>', '<|infill_word|>']:\n",
    "            return True\n",
    "        if len(infills)==0: return True\n",
    "    \n",
    "    \n",
    "    return False\n",
    "\n",
    "def get_infilled_responses(context, sentence_infill_pairs, num_infills=1, verbose=False):\n",
    "    list_generated_responses = []\n",
    "    for o in sentence_infill_pairs:\n",
    "        infill_sent = get_infill_substitutes(o[1])\n",
    "        is_high_infill = test_pure_infill(infill_sent)\n",
    "        if is_high_infill:\n",
    "            continue\n",
    "#         print(o[1])\n",
    "        if verbose: print(infill_sent)\n",
    "        data_input = context + ' ' + infill_sent\n",
    "        context_ids = get_context_ids(data_input)\n",
    "#         print(context_ids)\n",
    "#         print(data_input)\n",
    "#         print(ilm.tokenize_util.decode(context_ids, tokenizer))\n",
    "    #     print('-----')\n",
    "\n",
    "        generated = infill_with_ilm(\n",
    "            model,\n",
    "            additional_tokens_to_ids,\n",
    "            context_ids,\n",
    "            num_infills=num_infills)\n",
    "        for g in generated:\n",
    "            generated_sent = ilm.tokenize_util.decode(g, tokenizer)\n",
    "            generated_response = generated_sent.split('<|response|> <|speaker2|> ')[-1]\n",
    "#             print('-' * 80)\n",
    "#             print(generated_sent)\n",
    "            if verbose: print(generated_response)\n",
    "            list_generated_responses.append(generated_response)\n",
    "        if verbose: print()\n",
    "#     random.shuffle(list_generated_responses)\n",
    "    \n",
    "    return list_generated_responses\n",
    "\n",
    "\n",
    "\n",
    "def get_infilled_responses_naive(context, sentence_infill_pairs, num_infills=2):\n",
    "    list_generated_responses = []\n",
    "    for o in sentence_infill_pairs:\n",
    "        infill_sent = get_infill_substitutes(o[1])\n",
    "#         print(infill_sent)\n",
    "        is_high_infill = test_pure_infill(infill_sent)\n",
    "        if is_high_infill:\n",
    "            continue\n",
    "#         print(o[1])\n",
    "#         print(infill_sent)\n",
    "        data_input = context + ' ' + infill_sent\n",
    "        context_ids = get_context_ids(data_input)\n",
    "#         print(data_input)\n",
    "        \n",
    "        generated = infill_naive_with_ilm(\n",
    "            model,\n",
    "            additional_tokens_to_ids,\n",
    "            context_ids,\n",
    "            num_infills=num_infills)\n",
    "        for g in generated:\n",
    "            generated_sent = ilm.tokenize_util.decode(g, tokenizer)\n",
    "            generated_response = generated_sent.split('<|startofinfill|>')[-1]\n",
    "#             print('-' * 80)\n",
    "#             print(generated_sent)\n",
    "#             print(generated_response)\n",
    "            if generated_response=='': continue\n",
    "            list_generated_responses.append(generated_response)\n",
    "#         print()\n",
    "#     random.shuffle(list_generated_responses)\n",
    "    \n",
    "    return list_generated_responses\n",
    "\n",
    "# [[r,o[1]] for r,o in zip(get_infilled_responses(context,outs), outs)]\n",
    "# responses_filled = get_infilled_responses_naive(context,outs)\n",
    "responses_filled = get_infilled_responses(context, outs, verbose=True)\n",
    "\n",
    "print(len(responses_filled))\n",
    "print([o[1] for o in outs])\n",
    "print(responses_filled)\n",
    "# [[r,o[1]] for r,o in zip(responses_filled, outs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import pprint\n",
    "import logging\n",
    "import requests\n",
    "import math\n",
    "import re\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "#https://www.scribendi.ai/comparing-bert-and-gpt-2-as-language-models-to-score-the-grammatical-correctness-of-a-sentence/\n",
    "#https://github.com/simonepri/lm-scorer\n",
    "p = re.compile(r'((?<=[\\.\\?!]\\s)(\\w+)|(^\\w+))')\n",
    "def cap(match):\n",
    "    return(match.group().capitalize())\n",
    "\n",
    "def read_json_data(filename):\n",
    "    data = []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "            \n",
    "    return data\n",
    "\n",
    "def get_context_string(doc, history_len = 4):\n",
    "    context_string = '<|context|>'\n",
    "    sents = doc['context'][-history_len:]\n",
    "    for i,sent in enumerate(sents):\n",
    "        context_string += ' <|speaker' + str((i-(len(sents)+1)%2)%2 +1) + '|> ' + sent\n",
    "    context_string += ' <|endofcontext|> <|response|> <|speaker2|> '\n",
    "#     doc = doc['response'][:]\n",
    "    \n",
    "    return context_string\n",
    "\n",
    "\n",
    "test_data = read_json_data('/home/ubuntu/Code/negaug/dataset/test.json')\n",
    "train_data = read_json_data('/home/ubuntu/Code/negaug/dataset/train.json')\n",
    "dev_data = read_json_data('/home/ubuntu/Code/negaug/dataset/dev.json')\n",
    "\n",
    "for data in dev_data:\n",
    "    data['context_string'] = get_context_string(data)\n",
    "for data in test_data:\n",
    "    data['context_string'] = get_context_string(data)\n",
    "for data in train_data:\n",
    "    data['context_string'] = get_context_string(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "738da7a1d1bd442a8f538e99aa91400f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1142.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now infill\n",
      "now filter\n",
      "now infill\n",
      "now filter\n",
      "now infill\n",
      "now filter\n",
      "now infill\n",
      "now filter\n",
      "now infill\n",
      "now filter\n",
      "now infill\n",
      "now filter\n",
      "now infill\n",
      "now filter\n",
      "now infill\n",
      "now filter\n",
      "now infill\n",
      "now filter\n",
      "now infill\n",
      "now filter\n"
     ]
    }
   ],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "test_data[0]\n",
    "\n",
    "def check_ispoor_text(sentence):\n",
    "    sentence_words = sentence.split()\n",
    "    \n",
    "    for i, word in enumerate(sentence_words):\n",
    "        if i>1 and word==sentence_words[i-1]:\n",
    "            return True\n",
    "        \n",
    "    return False\n",
    "\n",
    "def get_lm_ppl(sentences):\n",
    "    sentences = [p.sub(cap, r) for r in sentences]\n",
    "    scores = scorer.sentence_score(sentences, log=True)\n",
    "    scores_charlength = [s / (len(r)+2) for s,r in zip(scores, sentences)]\n",
    "    scores_length = [s / (len(r.split())+2) for s,r in zip(scores, sentences)]\n",
    "\n",
    "    return list(map(lambda x, y, z:(x,y,z), sentences, scores_charlength, scores_length)) \n",
    "\n",
    "\n",
    "def filter_clean_adv_set(adv_gen_neg_responses_t1, positive_responses, desired_number_responses = 5):\n",
    "    \n",
    "    #cleanup responses\n",
    "    adv_gen_neg_responses = []\n",
    "    for i, response in enumerate(adv_gen_neg_responses_t1):\n",
    "        clean_response = response.strip()\n",
    "        clean_response = clean_response.replace(' ,', ',')\n",
    "        clean_response = clean_response.replace(' .', '.')\n",
    "        clean_response = clean_response.replace(' ?', '?')\n",
    "        clean_response = clean_response.replace('  ', ' ')\n",
    "        clean_response = clean_response.replace(' !', '!')\n",
    "        clean_response = clean_response.replace(' \\' ', '\\'')\n",
    "        clean_response = clean_response.replace(' \\'', '\\'')\n",
    "        clean_response= re.sub(' +',' ',clean_response)\n",
    "        clean_response = re.sub('\\$ ', '$', re.sub(' \\%', '%', clean_response))\n",
    "        clean_response = re.sub('\\. (\\d+)', r'.\\1', clean_response)\n",
    "        clean_response = p.sub(cap, clean_response)\n",
    "#         clean_response = re.sub(r'($)?\\s+(\\d)(.)?\\s+(\\d)', r'\\1\\2', clean_response)\n",
    "        adv_gen_neg_responses.append(clean_response)\n",
    "    \n",
    "    adv_gen_neg_responses_t1 = adv_gen_neg_responses\n",
    "    \n",
    "    positive_responses_wordset_list = []\n",
    "    for pr in positive_responses:\n",
    "        positive_responses_wordset_list.append(set(pr.split()))\n",
    "    \n",
    "    #get lm scores \n",
    "    lm_scores = get_lm_ppl(adv_gen_neg_responses_t1)\n",
    "    \n",
    "    good_responses = []\n",
    "    responses_words_list = []\n",
    "    threshold = 0.1\n",
    "    sent_length_threshold = 7\n",
    "    tries = 0\n",
    "    while True:\n",
    "        tries+=1\n",
    "#         print('looped')\n",
    "        for i, response in enumerate(adv_gen_neg_responses_t1):\n",
    "            words_set = set(response.split())\n",
    "            max_sim_score = -0.1\n",
    "            #if sentence too small, decrease its chances\n",
    "            if len(words_set)<=sent_length_threshold:\n",
    "                max_sim_score = 0.89\n",
    "                \n",
    "            for old_response_words_set in responses_words_list:\n",
    "                intersection_words = old_response_words_set.intersection(words_set)\n",
    "                score = len(intersection_words)/len(words_set)\n",
    "                max_sim_score = max(score, max_sim_score)\n",
    "\n",
    "            #should not have too much overlap with positive responses\n",
    "            for pos_response_words_set in positive_responses_wordset_list:\n",
    "                interesection_words = pos_response_words_set.intersection(words_set)\n",
    "                score = len(interesection_words)/len(words_set)  \n",
    "#                 print(pos_response_words_set, words_set, score)\n",
    "                if score >=0.8:\n",
    "                    max_sim_score =0.89\n",
    "            \n",
    "            ##remove poor text, todo: add language model score\n",
    "            #if check_ispoor_text(response):\n",
    "            lm_sent = lm_scores[i][1]\n",
    "            lm_sent_wordlevel = lm_scores[i][2]\n",
    "#             print(lm_scores[i])\n",
    "            if lm_sent<-1.4 or lm_sent_wordlevel<-5.0:\n",
    "                max_sim_score = 0.89\n",
    "                     \n",
    "                \n",
    "            if max_sim_score==-0.1 or max_sim_score<threshold:\n",
    "#                 print('--addingresponse ', response, max_sim_score)\n",
    "                good_responses.append(response)\n",
    "                responses_words_list.append(words_set)\n",
    "                \n",
    "        threshold+=0.1\n",
    "        if sent_length_threshold>2:\n",
    "            sent_length_threshold-=1\n",
    "        \n",
    "        if len(good_responses)>=desired_number_responses:\n",
    "            return good_responses[:desired_number_responses]\n",
    "        \n",
    "        if tries>4 or threshold>0.99:\n",
    "            print('choosing randomly')\n",
    "            return random.sample(adv_gen_neg_responses_t1, desired_number_responses)\n",
    "        \n",
    "#         print(good_responses)\n",
    "    \n",
    "    return good_responses[:desired_number_responses]\n",
    "\n",
    "def get_maskable_candidates(positive_responses, context):\n",
    "    candidates = []\n",
    "    reserve = []\n",
    "    for pr in positive_responses:\n",
    "        prtokens = pr.lower().split()\n",
    "        prtokens = [t for t in prtokens if t.replace('.', '') not in stop_words]\n",
    "#         print(prtokens)\n",
    "        if len(prtokens)<2:\n",
    "            reserve.append(pr)\n",
    "        else:\n",
    "            candidates.append(pr)\n",
    "    \n",
    "    for pr in context:\n",
    "        prtokens = pr.lower().split()\n",
    "        prtokens = [t for t in prtokens if t not in stop_words]\n",
    "        if len(prtokens)<2:\n",
    "            reserve.append(pr)\n",
    "        else:\n",
    "            candidates.append(pr)\n",
    "    \n",
    "#     print(candidates, reserve)\n",
    "    return candidates + reserve\n",
    "\n",
    "\n",
    "def add_modgt_responses(test_data_point, train_data, verbose=False):\n",
    "    positive_responses = test_data_point['positive_responses']\n",
    "    context = test_data_point['context']\n",
    "    context_string = test_data_point['context_string']\n",
    "    #print(positive_responses)\n",
    "    masked_spans = []\n",
    "    candidates = get_maskable_candidates(context, positive_responses)\n",
    "    candidates = candidates[:len(positive_responses)]\n",
    "    for pr in candidates:\n",
    "        masked_spans += get_masked_spans([pr])\n",
    "    random_train = random.choice(train_data)\n",
    "    random_context = random_train['context']\n",
    "    random_context_string = random_train['context_string']\n",
    "    if verbose: print('random_context_string ',random_context_string)\n",
    "#     print('now infill')\n",
    "    adv_gen_neg_responses_t1 = get_infilled_responses(random_context_string, masked_spans, num_infills=3, verbose=verbose)\n",
    "#     adv_gen_neg_responses_t1 = get_infilled_responses_naive(random_context_string, masked_spans, num_infills=2)\n",
    "\n",
    "#     print(positive_responses)\n",
    "    if verbose: print(adv_gen_neg_responses_t1)\n",
    "#     print('now filter')\n",
    "    adv_gen_neg_responses_t1 = filter_clean_adv_set(adv_gen_neg_responses_t1, positive_responses)\n",
    "    test_data_point['adv_gen_neg_responses_t1'] = adv_gen_neg_responses_t1 \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# for i,data_point in enumerate(test_data):\n",
    "#     if i%100==0: print(i)\n",
    "#     if i==10: break\n",
    "#     print(i)\n",
    "# #     positive_responses = test_data['positive_responses']\n",
    "#     add_modgt_responses(data_point, train_data)\n",
    "# #     pprint.pprint(data_point, indent=1)\n",
    "# #     break\n",
    "\n",
    "# dest_file = \"/hdd/Code/Others/gitrepos/Dialogue-Evaluation-with-BERT/dataset/test_advt1.json\"\n",
    "# output_file = open(dest_file, 'w', encoding='utf-8')\n",
    "# for dic in train_data:\n",
    "#     json.dump(dic, output_file) \n",
    "#     output_file.write(\"\\n\")\n",
    "\n",
    "\n",
    "def save_data_to_disk(data, dest_file):\n",
    "    output_file = open(dest_file, 'w', encoding='utf-8')\n",
    "    for i,dic in enumerate(data):\n",
    "        if i==10:break\n",
    "        json.dump(dic, output_file) \n",
    "        output_file.write(\"\\n\")\n",
    "\n",
    "data = test_data\n",
    "dest_file = \"/home/ubuntu/Code/negaug/dataset/test_pp.json\"\n",
    "\n",
    "\n",
    "for i,data_point in tqdm(enumerate(data), total=len(data)):\n",
    "#     if i%100==0:\n",
    "#         print(i)\n",
    "    if i==10: break\n",
    "#     positive_responses = test_data['positive_responses']\n",
    "    add_modgt_responses(data_point, data)\n",
    "#     pprint.pprint(data_point, indent=1)\n",
    "\n",
    "save_data_to_disk(data, dest_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f62f58b4f3c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"en\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mall_stopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mall_stopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDeprecationWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"exists\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Path or Path-like to model data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "sp = spacy.load(\"en\")\n",
    "\n",
    "all_stopwords = sp.Defaults.stop_words\n",
    "all_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch-gen)",
   "language": "python",
   "name": "pytorch-gen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
